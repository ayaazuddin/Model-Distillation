{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation with Custom ResNet Models\n",
    "\n",
    "This notebook explores the implementation of knowledge distillation between custom ResNet models on the CIFAR-10 dataset. Knowledge distillation is a technique where a smaller model (student) learns from a larger model (teacher) to achieve better performance than it would when trained from scratch.\n",
    "\n",
    "We'll cover the following components:\n",
    "1. Model Architecture - Custom ResNet implementation\n",
    "2. Data Loading and Preprocessing\n",
    "3. Generalist Model Training\n",
    "4. Specialist Model Training with Knowledge Distillation\n",
    "5. Evaluation and Visualization\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Architecture\n",
    "\n",
    "We'll implement a custom ResNet architecture with configurable depth and width. The architecture consists of:\n",
    "\n",
    "- Basic residual blocks with skip connections\n",
    "- A custom ResNet class that allows for different widths and depths\n",
    "\n",
    "### 1.1 Basic Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class BasicResidualBlock(nn.Module):\n",
    "    \"\"\"Basic residual block for our custom ResNet\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                             stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                             stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection to match dimensions\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
    "                        stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        out += self.shortcut(residual)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Custom ResNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class CustomResNet(nn.Module):\n",
    "    \"\"\"Custom ResNet with configurable width and depth\"\"\"\n",
    "    \n",
    "    def __init__(self, num_blocks, width_factors, num_classes=10):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Create residual blocks with different widths\n",
    "        self.layer1 = self._make_layer(width_factors[0], num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(width_factors[1], num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width_factors[2], num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width_factors[3], num_blocks[3], stride=2)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(width_factors[3], num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _make_layer(self, width, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        \n",
    "        for stride in strides:\n",
    "            layers.append(BasicResidualBlock(self.in_channels, width, stride))\n",
    "            self.in_channels = width\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters in the model\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating Generalist and Specialist Models\n",
    "\n",
    "We'll define two functions to create our models:\n",
    "1. A larger \"generalist\" model that handles all 10 CIFAR-10 classes\n",
    "2. A smaller \"specialist\" model designed to handle only a subset of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create generalist model - deeper and wider\n",
    "def create_generalist_model(num_classes=10):\n",
    "    # More blocks and higher width factors for more parameters\n",
    "    num_blocks = [3, 4, 6, 3]  # Similar to ResNet-34\n",
    "    width_factors = [64, 128, 256, 512]  # Standard widths\n",
    "    \n",
    "    return CustomResNet(num_blocks, width_factors, num_classes)\n",
    "\n",
    "# Create specialist model - smaller than generalist\n",
    "def create_specialist_model(num_classes):\n",
    "    # Fewer blocks and smaller width factors for fewer parameters\n",
    "    num_blocks = [2, 2, 2, 2]  # Similar to ResNet-18 but smaller\n",
    "    width_factors = [32, 64, 128, 256]  # Half the standard widths\n",
    "    \n",
    "    return CustomResNet(num_blocks, width_factors, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Now we'll set up data loading for the CIFAR-10 dataset with appropriate augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data loading and preprocessing for CIFAR-10\n",
    "def load_cifar10():\n",
    "    # Data normalization for CIFAR-10\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                          download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                           shuffle=True, num_workers=4)\n",
    "    \n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                         download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                          shuffle=False, num_workers=4)\n",
    "    \n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    return trainloader, testloader, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and Evaluation Functions\n",
    "\n",
    "### 3.1 Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create plotting functions for training visualization\n",
    "def plot_training_progress(train_losses, train_accs, val_losses=None, val_accs=None, title=\"Training Progress\"):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "    if val_losses:\n",
    "        ax1.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracies\n",
    "    ax2.plot(epochs, train_accs, 'b-', label='Training Accuracy')\n",
    "    if val_accs:\n",
    "        ax2.plot(epochs, val_accs, 'r-', label='Validation Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Generalist Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train function for models with learning rate scheduling\n",
    "def train_model(model, trainloader, testloader, epochs=90, lr=0.1, model_name=\"Generalist\"):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Track metrics\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for i, data in enumerate(progress_bar):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': running_loss/(i+1), \n",
    "                'acc': 100.*correct/total,\n",
    "                'lr': scheduler.get_last_lr()[0]\n",
    "            })\n",
    "        \n",
    "        train_loss = running_loss/len(trainloader)\n",
    "        train_acc = 100.*correct/total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc = evaluate_model(model, testloader, return_metrics=True)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'{model_name}_best.pth')\n",
    "        \n",
    "        # Print progress\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Epoch: {epoch+1}/{epochs} | '\n",
    "              f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}% | '\n",
    "              f'Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.2f}% | '\n",
    "              f'Time: {elapsed/60:.2f}m | LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Plot every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "            fig = plot_training_progress(train_losses, train_accs, val_losses, val_accs, \n",
    "                                      title=f'{model_name} Training Progress')\n",
    "            plt.savefig(f'{model_name}_progress_epoch_{epoch+1}.png')\n",
    "            plt.close(fig)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(f'{model_name}_best.pth'))\n",
    "    \n",
    "    # Final plot\n",
    "    fig = plot_training_progress(train_losses, train_accs, val_losses, val_accs, \n",
    "                              title=f'{model_name} Training Progress (Final)')\n",
    "    plt.savefig(f'{model_name}_progress_final.png')\n",
    "    \n",
    "    # Print training summary\n",
    "    print(f'\\n{model_name} Training completed in {elapsed/60:.2f} minutes')\n",
    "    print(f'Best validation accuracy: {best_acc:.2f}%')\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'best_acc': best_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate model with option to return metrics\n",
    "def evaluate_model(model, testloader, return_metrics=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    test_loss = running_loss / len(testloader)\n",
    "    \n",
    "    if not return_metrics:\n",
    "        print(f'Accuracy on test set: {accuracy:.2f}%')\n",
    "        return accuracy\n",
    "    else:\n",
    "        return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Knowledge Distillation Implementation\n",
    "\n",
    "### 4.1 Distillation Loss Function\n",
    "This function implements the knowledge distillation loss, which combines:\n",
    "1. A hard loss on the actual labels\n",
    "2. A soft loss based on the teacher model's logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Knowledge distillation loss function\n",
    "def distillation_loss(outputs, labels, teacher_outputs, class_mapping, T=2.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Modified distillation loss to handle different output dimensions\n",
    "    \n",
    "    Args:\n",
    "        outputs: specialist model outputs (e.g., 4 classes)\n",
    "        labels: remapped labels for specialist classes\n",
    "        teacher_outputs: generalist model outputs (10 classes)\n",
    "        class_mapping: maps original classes to specialist classes\n",
    "        T: temperature for distillation\n",
    "        alpha: weight for hard vs soft loss\n",
    "    \"\"\"\n",
    "    # Hard loss with actual labels\n",
    "    hard_loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "    \n",
    "    # Extract only the relevant logits from teacher outputs\n",
    "    # Create reverse mapping\n",
    "    reverse_mapping = {v: k for k, v in class_mapping.items()}\n",
    "    specialist_class_indices = list(class_mapping.keys())\n",
    "    \n",
    "    # Extract only relevant teacher logits\n",
    "    teacher_relevant_logits = torch.zeros_like(outputs).to(outputs.device)\n",
    "    batch_size = outputs.size(0)\n",
    "    \n",
    "    for i in range(len(reverse_mapping)):\n",
    "        original_idx = reverse_mapping[i]\n",
    "        teacher_relevant_logits[:, i] = teacher_outputs[:, original_idx]\n",
    "    \n",
    "    # Compute soft targets KL divergence\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "        nn.functional.log_softmax(outputs/T, dim=1),\n",
    "        nn.functional.softmax(teacher_relevant_logits/T, dim=1)\n",
    "    ) * (T * T)\n",
    "    \n",
    "    return alpha * hard_loss + (1 - alpha) * soft_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Specialist Model Training with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train specialist models with knowledge distillation\n",
    "def train_specialist_with_distillation(specialist_model, generalist_model, trainloader, testloader,\n",
    "                                     class_mapping, epochs=90, lr=0.1, model_name=\"Specialist\"):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(specialist_model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    specialist_model.to(device)\n",
    "    generalist_model.to(device)\n",
    "    generalist_model.eval()  # Set teacher model to evaluation mode\n",
    "    \n",
    "    # Track metrics\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        specialist_model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for i, data in enumerate(progress_bar):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Remap labels for specialist classes\n",
    "            specialist_labels = torch.tensor([class_mapping[l.item()] for l in labels], \n",
    "                                          device=device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get student outputs\n",
    "            student_outputs = specialist_model(inputs)\n",
    "            \n",
    "            # Get teacher outputs\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = generalist_model(inputs)\n",
    "            \n",
    "            # Calculate distillation loss\n",
    "            loss = distillation_loss(student_outputs, specialist_labels, teacher_outputs, class_mapping)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = student_outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(specialist_labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': running_loss/(i+1), \n",
    "                'acc': 100.*correct/total,\n",
    "                'lr': scheduler.get_last_lr()[0]\n",
    "            })\n",
    "        \n",
    "        train_loss = running_loss/len(trainloader)\n",
    "        train_acc = 100.*correct/total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc = evaluate_specialist(\n",
    "            specialist_model, testloader, class_mapping, return_metrics=True\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(specialist_model.state_dict(), f'{model_name}_best.pth')\n",
    "        \n",
    "        # Print progress\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Epoch: {epoch+1}/{epochs} | '\n",
    "              f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}% | '\n",
    "              f'Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.2f}% | '\n",
    "              f'Time: {elapsed/60:.2f}m | LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Plot every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "            fig = plot_training_progress(train_losses, train_accs, val_losses, val_accs, \n",
    "                                       title=f'{model_name} Training Progress')\n",
    "            plt.savefig(f'{model_name}_progress_epoch_{epoch+1}.png')\n",
    "            plt.close(fig)\n",
    "    \n",
    "    # Load best model\n",
    "    specialist_model.load_state_dict(torch.load(f'{model_name}_best.pth'))\n",
    "    \n",
    "    # Final plot\n",
    "    fig = plot_training_progress(train_losses, train_accs, val_losses, val_accs, \n",
    "                               title=f'{model_name} Training Progress (Final)')\n",
    "    plt.savefig(f'{model_name}_progress_final.png')\n",
    "    \n",
    "    # Print training summary\n",
    "    print(f'\\n{model_name} Training completed in {elapsed/60:.2f} minutes')\n",
    "    print(f'Best validation accuracy: {best_acc:.2f}%')\n",
    "    \n",
    "    return specialist_model, {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'best_acc': best_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Specialist Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate specialist model\n",
    "def evaluate_specialist(specialist_model, testloader, class_mapping, return_metrics=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    specialist_model.to(device)\n",
    "    specialist_model.eval()\n",
    "    \n",
    "    # Create reverse mapping\n",
    "    reverse_mapping = {v: k for k, v in class_mapping.items()}\n",
    "    class_subset = list(class_mapping.keys())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Filter only samples belonging to specialist classes\n",
    "            mask = torch.tensor([l.item() in class_subset for l in labels], device=device)\n",
    "            if not mask.any():\n",
    "                continue\n",
    "            \n",
    "            specialist_images = images[mask]\n",
    "            original_labels = labels[mask]\n",
    "            \n",
    "            # Remap labels for specialist \n",
    "            specialist_labels = torch.tensor([class_mapping[l.item()] for l in original_labels], \n",
    "                                          device=device)\n",
    "            \n",
    "            outputs = specialist_model(specialist_images)\n",
    "            loss = criterion(outputs, specialist_labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += specialist_labels.size(0)\n",
    "            correct += (predicted == specialist_labels).sum().item()\n",
    "    \n",
    "    if total == 0:  # Avoid division by zero\n",
    "        accuracy = 0\n",
    "        test_loss = 0\n",
    "    else:\n",
    "        accuracy = 100 * correct / total\n",
    "        test_loss = running_loss / (len(testloader) * len(class_subset) / 10)\n",
    "    \n",
    "    if not return_metrics:\n",
    "        print(f'Specialist accuracy on relevant test set: {accuracy:.2f}%')\n",
    "        return accuracy\n",
    "    else:\n",
    "        return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Training Process\n",
    "\n",
    "Finally, let's put everything together in the main function that runs the entire knowledge distillation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Main function to run the distillation process\n",
    "def main():\n",
    "    # Create output directory\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    \n",
    "    # Load data\n",
    "    trainloader, testloader, classes = load_cifar10()\n",
    "    \n",
    "    # Define class groups for specialists\n",
    "    class_groups = [\n",
    "        [2, 3, 4, 5],  # Bird, Cat, Deer, Dog (animals)\n",
    "        [0, 1, 8, 9]   # Plane, Car, Ship, Truck (vehicles)\n",
    "    ]\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Step 1: Create and train generalist model\n",
    "    print(\"Creating generalist model...\")\n",
    "    generalist_model = create_generalist_model(num_classes=10)\n",
    "    print(f\"Generalist model parameters: {generalist_model.count_parameters():,}\")\n",
    "    \n",
    "    print(\"Training generalist model...\")\n",
    "    generalist_model, gen_metrics = train_model(\n",
    "        generalist_model, trainloader, testloader, epochs=10, lr=0.1, model_name=\"Generalist\"\n",
    "    )\n",
    "    gen_accuracy = gen_metrics['best_acc']\n",
    "    \n",
    "    # Step 2: Train specialist models with knowledge distillation\n",
    "    specialist_models = []\n",
    "    class_mappings = []\n",
    "    specialist_metrics = []\n",
    "    \n",
    "    for i, group in enumerate(class_groups):\n",
    "        print(f\"Training specialist model {i+1} for classes {group}...\")\n",
    "        \n",
    "        # Create class mapping for this specialist\n",
    "        class_mapping = {original_class: idx for idx, original_class in enumerate(group)}\n",
    "        class_mappings.append(class_mapping)\n",
    "        \n",
    "        # Create dataset subset\n",
    "        indices = []\n",
    "        for j, (_, target) in enumerate(trainloader.dataset):\n",
    "            if target in group:\n",
    "                indices.append(j)\n",
    "        \n",
    "        subset = torch.utils.data.Subset(trainloader.dataset, indices)\n",
    "        specialist_loader = torch.utils.data.DataLoader(\n",
    "            subset, batch_size=128, shuffle=True, num_workers=4\n",
    "        )\n",
    "        \n",
    "        # Create smaller specialist model\n",
    "        specialist_model = create_specialist_model(num_classes=len(group))\n",
    "        print(f\"Specialist model {i+1} parameters: {specialist_model.count_parameters():,}\")\n",
    "        \n",
    "        # Train specialist model\n",
    "        specialist_model, spec_metrics = train_specialist_with_distillation(\n",
    "            specialist_model, generalist_model, specialist_loader, testloader, \n",
    "            class_mapping, epochs=10, lr=0.1, model_name=f\"Specialist_{i+1}\"\n",
    "        )\n",
    "        specialist_models.append(specialist_model)\n",
    "        specialist_metrics.append(spec_metrics)\n",
    "    \n",
    "    # Save the trained models\n",
    "    print(\"\\nSaving all models...\")\n",
    "    \n",
    "    # Save generalist model\n",
    "    torch.save({\n",
    "        'model_state_dict': generalist_model.state_dict(),\n",
    "        'accuracy': gen_accuracy\n",
    "    }, 'models/generalist_model_complete.pth')\n",
    "    \n",
    "    # Save specialist models\n",
    "    for i, model in enumerate(specialist_models):\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'accuracy': specialist_metrics[i]['best_acc'],\n",
    "            'class_mapping': class_mappings[i],\n",
    "            'class_group': class_groups[i]\n",
    "        }, f'models/specialist_model_{i+1}_complete.pth')\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Training Pipeline\n",
    "\n",
    "Now let's execute our main function to train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In this notebook, we've implemented knowledge distillation with custom ResNet models on the CIFAR-10 dataset. Here's a recap of what we've done:\n",
    "\n",
    "1. **Architecture**: We created a custom ResNet architecture with configurable depth and width.\n",
    "   \n",
    "2. **Models**:\n",
    "   - **Generalist Model**: A larger model trained on all 10 CIFAR-10 classes\n",
    "   - **Specialist Models**: Smaller models trained on specific subsets of classes\n",
    "   \n",
    "3. **Knowledge Distillation**:\n",
    "   - Used the generalist model as a teacher to train specialist models\n",
    "   - Implemented a modified distillation loss that handles different output dimensions\n",
    "   - The specialist models learn from both ground truth labels and the generalist model's soft targets\n",
    "   \n",
    "4. **Evaluation**:\n",
    "   - The specialist models are evaluated only on their specific subset of classes\n",
    "   - Training progress is visualized with plots of loss and accuracy\n",
    "\n",
    "Knowledge distillation is a powerful technique for model compression and specialization. By using a larger teacher model to guide the training of smaller student models, we can create more efficient models for specific tasks while maintaining good performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}